{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc\n",
    "import data_utils as utils\n",
    "import Model\n",
    "import ValidationSystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('train.json')\n",
    "#train = train.loc[:40].copy()\n",
    "\n",
    "labelForTrain = np.array(train['is_iceberg'])\n",
    "band1 = utils.unflattenBand(train['band_1'])\n",
    "band2 = utils.unflattenBand(train['band_2'])\n",
    "del train\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_json('test.json')\n",
    "#test = test.loc[:10].copy()\n",
    "#predictionsForSubmission = test[[\"id\"]].copy()\n",
    "#test.to_json(\"test2.json\",orient='records')\n",
    "\n",
    "band1test = utils.unflattenBand(test['band_1'])\n",
    "band2test = utils.unflattenBand(test['band_2'])\n",
    "ids = np.array(test['id'])\n",
    "del test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "1th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1402\n",
      "Val size: 202\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.875/0.289 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.782/0.731\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.977/0.100 ---\n",
      "[Epoch 51/300] VAL   acc/loss: 0.906/0.205\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.969/0.107 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.896/0.233\n",
      "----------------------------------------------\n",
      "*******************************Epoch   100: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.984/0.038 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.906/0.348\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  106\n",
      "Best Val acc: 0.9306930693069307, Val loss: 0.1718116314104288\n",
      "Test acc: 0.2955840455840456, Test loss: 4.313513149223437\n",
      "--- 572.2203664779663 seconds for this training\n",
      "##############################################\n",
      "2th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.930/0.158 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.871/0.395\n",
      "----------------------------------------------\n",
      "*******************************Epoch    40: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.992/0.015 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.915/0.349\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  60\n",
      "Best Val acc: 0.9054726368159204, Val loss: 0.2108689550847853\n",
      "Test acc: 0.49145299145299143, Test loss: 2.59657637266918\n",
      "--- 326.1776633262634 seconds for this training\n",
      "##############################################\n",
      "3th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.914/0.205 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.881/0.304\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.938/0.150 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.925/0.202\n",
      "Best val_acc: 0.925373\n",
      "Best val_loss: 0.202410\n",
      "----------------------------------------------\n",
      "*******************************Epoch    52: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.945/0.082 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.910/0.283\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  83\n",
      "Best Val acc: 0.9253731343283582, Val loss: 0.17757715490549358\n",
      "Test acc: 0.39886039886039887, Test loss: 3.813966503396899\n",
      "--- 439.013112783432 seconds for this training\n",
      "##############################################\n",
      "4th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.891/0.246 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.890/0.278\n",
      "Best val_acc: 0.890000\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.961/0.144 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.875/0.412\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.969/0.078 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.895/0.321\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.992/0.028 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.905/0.350\n",
      "----------------------------------------------\n",
      "Early stopped!!No increase in val loss.., total epoch is:  121\n",
      "Best Val acc: 0.9, Val loss: 0.21819988012313843\n",
      "Test acc: 0.3986229819563153, Test loss: 2.717016225413606\n",
      "--- 624.1240134239197 seconds for this training\n",
      "##############################################\n",
      "5th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.922/0.192 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.930/0.200\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.969/0.069 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.900/0.277\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.977/0.045 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.930/0.280\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  86\n",
      "Best Val acc: 0.915, Val loss: 0.16690240144729615\n",
      "Test acc: 0.6635802469135802, Test loss: 2.089258169969376\n",
      "--- 450.7112510204315 seconds for this training\n",
      "##############################################\n",
      "6th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.914/0.182 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.895/0.264\n",
      "Best val_loss: 0.264212\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.914/0.145 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.860/0.441\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.977/0.061 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.775/0.555\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.961/0.088 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.895/0.467\n",
      "----------------------------------------------\n",
      "Early stopped!!No increase in val loss.., total epoch is:  105\n",
      "Best Val acc: 0.895, Val loss: 0.2642118275165558\n",
      "Test acc: 0.34199905033238365, Test loss: 3.7452245743526014\n",
      "--- 548.773179769516 seconds for this training\n",
      "##############################################\n",
      "7th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.953/0.141 ---\n",
      "[Epoch 26/300] VAL   acc/loss: 0.845/0.333\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.992/0.038 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.875/0.336\n",
      "----------------------------------------------\n",
      "*******************************Epoch    70: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.961/0.089 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.875/0.473\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  81\n",
      "Best Val acc: 0.885, Val loss: 0.285966095328331\n",
      "Test acc: 0.6735517568850902, Test loss: 1.2208213263880268\n",
      "--- 425.88967776298523 seconds for this training\n",
      "##############################################\n",
      "8th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.906/0.163 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.780/0.516\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.953/0.094 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.875/0.320\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.992/0.018 ---\n",
      "[Epoch 76/300] VAL   acc/loss: 0.875/0.376\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.992/0.016 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.915/0.299\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  115\n",
      "Best Val acc: 0.935, Val loss: 0.20943181455135346\n",
      "Test acc: 0.48444919278252613, Test loss: 2.185626258990477\n",
      "--- 597.5575575828552 seconds for this training\n",
      "   is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  is_iceberg_3  \\\n",
      "0         0.0      0.097573      0.004389      0.189438      0.023042   \n",
      "1         0.0      0.997874      0.009587      0.993130      0.987934   \n",
      "2         0.0      0.042839      0.018220      0.080530      0.054793   \n",
      "3         0.0      0.999270      0.999849      0.999760      0.998190   \n",
      "4         0.0      0.999767      0.998782      0.999895      0.998083   \n",
      "5         0.0      0.995490      0.500697      0.976626      0.980002   \n",
      "6         0.0      0.042137      0.003012      0.000186      0.097551   \n",
      "7         0.0      0.998429      0.999597      0.999658      0.998087   \n",
      "8         0.0      0.009894      0.013763      0.000543      0.012107   \n",
      "9         0.0      0.002580      0.001399      0.000280      0.002590   \n",
      "\n",
      "   is_iceberg_4  is_iceberg_5  is_iceberg_6  is_iceberg_7  \n",
      "0      0.085305      0.006115      0.071878      0.012369  \n",
      "1      0.067791      0.995764      0.149967      0.063118  \n",
      "2      0.001442      0.269638      0.598318      0.007377  \n",
      "3      1.000000      0.999419      0.995867      0.999533  \n",
      "4      0.976964      0.999546      0.048606      0.986939  \n",
      "5      0.001421      0.976994      0.798289      0.921196  \n",
      "6      0.068850      0.064101      0.041161      0.065045  \n",
      "7      0.999998      0.999305      0.994831      0.999124  \n",
      "8      0.019087      0.000806      0.007905      0.000056  \n",
      "9      0.000483      0.000082      0.002588      0.000011  \n",
      "              is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  \\\n",
      "is_iceberg           NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0         NaN      1.000000      0.681710      0.796833   \n",
      "is_iceberg_1         NaN      0.681710      1.000000      0.812723   \n",
      "is_iceberg_2         NaN      0.796833      0.812723      1.000000   \n",
      "is_iceberg_3         NaN      0.768821      0.776489      0.852236   \n",
      "is_iceberg_4         NaN      0.430416      0.683012      0.578079   \n",
      "is_iceberg_5         NaN      0.852611      0.776994      0.850620   \n",
      "is_iceberg_6         NaN      0.431682      0.570427      0.523557   \n",
      "is_iceberg_7         NaN      0.660128      0.829877      0.783706   \n",
      "\n",
      "              is_iceberg_3  is_iceberg_4  is_iceberg_5  is_iceberg_6  \\\n",
      "is_iceberg             NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0      0.768821      0.430416      0.852611      0.431682   \n",
      "is_iceberg_1      0.776489      0.683012      0.776994      0.570427   \n",
      "is_iceberg_2      0.852236      0.578079      0.850620      0.523557   \n",
      "is_iceberg_3      1.000000      0.641412      0.876307      0.594114   \n",
      "is_iceberg_4      0.641412      1.000000      0.573857      0.755151   \n",
      "is_iceberg_5      0.876307      0.573857      1.000000      0.530249   \n",
      "is_iceberg_6      0.594114      0.755151      0.530249      1.000000   \n",
      "is_iceberg_7      0.823975      0.735886      0.775300      0.610163   \n",
      "\n",
      "              is_iceberg_7  \n",
      "is_iceberg             NaN  \n",
      "is_iceberg_0      0.660128  \n",
      "is_iceberg_1      0.829877  \n",
      "is_iceberg_2      0.783706  \n",
      "is_iceberg_3      0.823975  \n",
      "is_iceberg_4      0.735886  \n",
      "is_iceberg_5      0.775300  \n",
      "is_iceberg_6      0.610163  \n",
      "is_iceberg_7      1.000000  \n",
      "\n",
      "epochForBestValLoss:  [36, 31, 65, 42, 16, 26, 24, 44]\n",
      "testAccuracy:  [0.29558404558404561, 0.49145299145299143, 0.39886039886039887, 0.39862298195631529, 0.6635802469135802, 0.34199905033238365, 0.67355175688509017, 0.48444919278252613]\n",
      "valLossForBestValLoss:  [0.17181163141042879, 0.21086895508478529, 0.17757715490549358, 0.21819988012313843, 0.16690240144729615, 0.26421182751655581, 0.285966095328331, 0.20943181455135346]\n",
      "testLoss:  [4.3135131492234366, 2.5965763726691802, 3.813966503396899, 2.7170162254136061, 2.089258169969376, 3.7452245743526014, 1.2208213263880268, 2.1856262589904771]\n",
      "\n",
      "Mean of epochForBestValLoss:  35.5\n",
      "Mean of testAccuracy:  0.468512583096\n",
      "Mean of valLossForBestValLoss:  0.213121220046\n",
      "Mean of testLoss:  2.83525032255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "1th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1402\n",
      "Val size: 202\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.914/0.147 ---\n",
      "[Epoch 26/300] VAL   acc/loss: 0.866/0.291\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.984/0.052 ---\n",
      "[Epoch 51/300] VAL   acc/loss: 0.916/0.199\n",
      "----------------------------------------------\n",
      "*******************************Epoch    75: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.969/0.062 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.881/0.348\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  85\n",
      "Best Val acc: 0.9356435643564357, Val loss: 0.17744086417231228\n",
      "Test acc: 0.47708926875593544, Test loss: 2.381906259999542\n",
      "--- 452.736008644104 seconds for this training\n",
      "##############################################\n",
      "2th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.562/0.682 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.532/0.688\n",
      "----------------------------------------------\n",
      "*******************************Epoch    29: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.906/0.276 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.876/0.304\n",
      "Best val_acc: 0.875622\n",
      "Best val_loss: 0.304238\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.898/0.199 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.861/0.388\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.945/0.101 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.905/0.267\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 1.000/0.009 ---\n",
      "[Epoch 126/300] VAL   acc/loss: 0.871/0.492\n",
      "----------------------------------------------\n",
      "*******************************Epoch   146: reducing learning rate of group 0 to 5.5556e-04.\n",
      "train loss is below  0.0075 , total epoch is:  150\n",
      "Best Val acc: 0.9154228855721394, Val loss: 0.2123018937250275\n",
      "Test acc: 0.7126068376068376, Test loss: 1.162665382862544\n",
      "--- 778.5797388553619 seconds for this training\n",
      "##############################################\n",
      "3th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.891/0.236 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.871/0.323\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.977/0.108 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.900/0.217\n",
      "----------------------------------------------\n",
      "*******************************Epoch    75: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.953/0.093 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.891/0.322\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  91\n",
      "Best Val acc: 0.9104477611940298, Val loss: 0.2105849982790686\n",
      "Test acc: 0.5022554605887939, Test loss: 2.181782220277125\n",
      "--- 502.30573654174805 seconds for this training\n",
      "##############################################\n",
      "4th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.898/0.202 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.850/0.369\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.969/0.059 ---\n",
      "[Epoch 51/300] VAL   acc/loss: 0.880/0.324\n",
      "----------------------------------------------\n",
      "*******************************Epoch    71: reducing learning rate of group 0 to 1.6667e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.977/0.073 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.920/0.288\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  76\n",
      "Best Val acc: 0.925, Val loss: 0.16587672472000123\n",
      "Test acc: 0.36182336182336183, Test loss: 3.564768839652269\n",
      "--- 432.201030254364 seconds for this training\n",
      "##############################################\n",
      "5th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.891/0.250 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.900/0.211\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.922/0.189 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.825/0.598\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.953/0.156 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.935/0.162\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.992/0.054 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.945/0.194\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.984/0.048 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.895/0.536\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  130\n",
      "Best Val acc: 0.95, Val loss: 0.11742973908782005\n",
      "Test acc: 0.413698955365622, Test loss: 3.14276825577898\n",
      "--- 719.8204743862152 seconds for this training\n",
      "##############################################\n",
      "6th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.938/0.160 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.865/0.354\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.961/0.075 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.855/0.424\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.992/0.050 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.885/0.434\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.992/0.030 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.865/0.571\n",
      "----------------------------------------------\n",
      "Early stopped!!No increase in val loss.., total epoch is:  104\n",
      "Best Val acc: 0.885, Val loss: 0.2794654306769371\n",
      "Test acc: 0.7076210826210826, Test loss: 1.1801755415861197\n",
      "--- 562.5915637016296 seconds for this training\n",
      "##############################################\n",
      "7th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.914/0.209 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.815/0.487\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.961/0.087 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.845/0.349\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.953/0.145 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.885/0.357\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.969/0.080 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.800/0.982\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  103\n",
      "Best Val acc: 0.875, Val loss: 0.26587148070335387\n",
      "Test acc: 0.37298195631528963, Test loss: 3.5133699175299404\n",
      "--- 547.4988188743591 seconds for this training\n",
      "##############################################\n",
      "8th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.836/0.344 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.820/0.341\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.922/0.210 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.875/0.373\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.961/0.126 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.855/0.380\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.969/0.120 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.865/0.330\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.977/0.061 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.895/0.386\n",
      "----------------------------------------------\n",
      "*******************************Epoch   127: reducing learning rate of group 0 to 1.6667e-03.\n",
      "train loss is below  0.0075 , total epoch is:  133\n",
      "Best Val acc: 0.91, Val loss: 0.22676595374941827\n",
      "Test acc: 0.4732905982905983, Test loss: 2.7135450311309364\n",
      "--- 698.3757801055908 seconds for this training\n",
      "   is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  is_iceberg_3  \\\n",
      "0         0.0      0.005817      0.012166      0.021885  9.764768e-03   \n",
      "1         0.0      0.936172      0.046231      0.584832  9.792884e-01   \n",
      "2         0.0      0.461368      0.026828      0.834661  8.529713e-04   \n",
      "3         0.0      0.998949      0.997113      0.999348  9.997447e-01   \n",
      "4         0.0      0.998889      0.006803      0.423380  9.997013e-01   \n",
      "5         0.0      0.994630      0.977070      0.993311  9.974241e-01   \n",
      "6         0.0      0.105430      0.029096      0.062049  3.677304e-01   \n",
      "7         0.0      0.998889      0.996981      0.999337  9.997399e-01   \n",
      "8         0.0      0.060110      0.002248      0.010796  5.255540e-03   \n",
      "9         0.0      0.000016      0.000712      0.000227  4.965041e-07   \n",
      "\n",
      "   is_iceberg_4  is_iceberg_5  is_iceberg_6  is_iceberg_7  \n",
      "0      0.011201      0.000623      0.011962  3.564462e-05  \n",
      "1      0.379671      0.313964      0.827305  8.350157e-01  \n",
      "2      0.015371      0.010567      0.295770  7.429876e-10  \n",
      "3      0.999559      0.999342      0.999787  9.998829e-01  \n",
      "4      0.998579      0.093388      0.999712  9.322609e-01  \n",
      "5      0.645178      0.939707      0.984356  9.732624e-01  \n",
      "6      0.017182      0.075132      0.043817  1.110946e-03  \n",
      "7      0.999549      0.999346      0.999809  9.997937e-01  \n",
      "8      0.000038      0.000578      0.031408  4.406783e-05  \n",
      "9      0.000062      0.000006      0.000299  3.856084e-07  \n",
      "              is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  \\\n",
      "is_iceberg           NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0         NaN      1.000000      0.654730      0.745006   \n",
      "is_iceberg_1         NaN      0.654730      1.000000      0.685783   \n",
      "is_iceberg_2         NaN      0.745006      0.685783      1.000000   \n",
      "is_iceberg_3         NaN      0.880068      0.549399      0.662059   \n",
      "is_iceberg_4         NaN      0.830994      0.552304      0.676081   \n",
      "is_iceberg_5         NaN      0.687215      0.856906      0.671046   \n",
      "is_iceberg_6         NaN      0.829276      0.506342      0.690059   \n",
      "is_iceberg_7         NaN      0.855096      0.618644      0.724078   \n",
      "\n",
      "              is_iceberg_3  is_iceberg_4  is_iceberg_5  is_iceberg_6  \\\n",
      "is_iceberg             NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0      0.880068      0.830994      0.687215      0.829276   \n",
      "is_iceberg_1      0.549399      0.552304      0.856906      0.506342   \n",
      "is_iceberg_2      0.662059      0.676081      0.671046      0.690059   \n",
      "is_iceberg_3      1.000000      0.860735      0.580937      0.837674   \n",
      "is_iceberg_4      0.860735      1.000000      0.554975      0.832472   \n",
      "is_iceberg_5      0.580937      0.554975      1.000000      0.513302   \n",
      "is_iceberg_6      0.837674      0.832472      0.513302      1.000000   \n",
      "is_iceberg_7      0.825654      0.803615      0.651540      0.817319   \n",
      "\n",
      "              is_iceberg_7  \n",
      "is_iceberg             NaN  \n",
      "is_iceberg_0      0.855096  \n",
      "is_iceberg_1      0.618644  \n",
      "is_iceberg_2      0.724078  \n",
      "is_iceberg_3      0.825654  \n",
      "is_iceberg_4      0.803615  \n",
      "is_iceberg_5      0.651540  \n",
      "is_iceberg_6      0.817319  \n",
      "is_iceberg_7      1.000000  \n",
      "\n",
      "epochForBestValLoss:  [33, 74, 40, 30, 75, 25, 62, 111]\n",
      "testAccuracy:  [0.47708926875593544, 0.71260683760683763, 0.50225546058879389, 0.36182336182336183, 0.41369895536562201, 0.70762108262108259, 0.37298195631528963, 0.47329059829059827]\n",
      "valLossForBestValLoss:  [0.17744086417231228, 0.2123018937250275, 0.2105849982790686, 0.16587672472000123, 0.11742973908782005, 0.27946543067693708, 0.26587148070335387, 0.22676595374941827]\n",
      "testLoss:  [2.3819062599995422, 1.162665382862544, 2.1817822202771251, 3.564768839652269, 3.14276825577898, 1.1801755415861197, 3.5133699175299404, 2.7135450311309364]\n",
      "\n",
      "Mean of epochForBestValLoss:  56.25\n",
      "Mean of testAccuracy:  0.502670940171\n",
      "Mean of valLossForBestValLoss:  0.206967135639\n",
      "Mean of testLoss:  2.4801226811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################\n",
      "1th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1402\n",
      "Val size: 202\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.945/0.156 ---\n",
      "[Epoch 26/300] VAL   acc/loss: 0.896/0.246\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.938/0.205 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.906/0.230\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.945/0.116 ---\n",
      "[Epoch 76/300] VAL   acc/loss: 0.797/0.515\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.938/0.142 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.861/0.326\n",
      "----------------------------------------------\n",
      "*******************************Epoch   108: reducing learning rate of group 0 to 3.3333e-03.\n",
      "Early stopped!!No increase in val loss.., total epoch is:  118\n",
      "Best Val acc: 0.9108910891089109, Val loss: 0.18662371198729713\n",
      "Test acc: 0.7152184235517569, Test loss: 1.0057658977091934\n",
      "--- 646.3955521583557 seconds for this training\n",
      "##############################################\n",
      "2th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.914/0.234 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.905/0.253\n",
      "Best val_acc: 0.905473\n",
      "Best val_loss: 0.252932\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.891/0.271 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.900/0.334\n",
      "----------------------------------------------\n",
      "*******************************Epoch    51: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.945/0.119 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.905/0.269\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.984/0.043 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.896/0.309\n",
      "----------------------------------------------\n",
      "*******************************Epoch   125: reducing learning rate of group 0 to 1.1111e-03.\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.984/0.043 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.925/0.292\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  135\n",
      "Best Val acc: 0.9353233830845771, Val loss: 0.2030802727375754\n",
      "Test acc: 0.4648622981956315, Test loss: 3.214244129764162\n",
      "--- 752.9082584381104 seconds for this training\n",
      "##############################################\n",
      "3th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1403\n",
      "Val size: 201\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.930/0.159 ---\n",
      "[Epoch 26/300] VAL   acc/loss: 0.861/0.251\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.930/0.135 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.935/0.196\n",
      "Best val_acc: 0.935323\n",
      "Best val_loss: 0.195832\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.969/0.101 ---\n",
      "[Epoch 76/300] VAL   acc/loss: 0.896/0.277\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.953/0.117 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.846/0.362\n",
      "----------------------------------------------\n",
      "*******************************Epoch   123: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.977/0.068 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.905/0.263\n",
      "----------------------------------------------\n",
      "Early stopped!!No increase in val loss.., total epoch is:  130\n",
      "Best Val acc: 0.9353233830845771, Val loss: 0.1958321082354778\n",
      "Test acc: 0.5303893637226971, Test loss: 2.0038305050061074\n",
      "--- 719.5221872329712 seconds for this training\n",
      "##############################################\n",
      "4th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.883/0.252 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.820/0.430\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.945/0.165 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.860/0.272\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.922/0.164 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.875/0.368\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.945/0.139 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.890/0.362\n",
      "----------------------------------------------\n",
      "*******************************Epoch   111: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 126/300]    TRAIN acc/loss: 1.000/0.022 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.925/0.226\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  136\n",
      "Best Val acc: 0.95, Val loss: 0.1690300750732422\n",
      "Test acc: 0.5150759734093068, Test loss: 3.0279866845293038\n",
      "--- 722.0540716648102 seconds for this training\n",
      "##############################################\n",
      "5th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.898/0.252 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.750/0.574\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.938/0.143 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.930/0.233\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.930/0.255 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.940/0.225\n",
      "----------------------------------------------\n",
      "*******************************Epoch    82: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.969/0.054 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.935/0.227\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 1.000/0.008 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.920/0.406\n",
      "----------------------------------------------\n",
      "Early stopped!!No increase in val loss.., total epoch is:  132\n",
      "Best Val acc: 0.945, Val loss: 0.15925306484103202\n",
      "Test acc: 0.3378442545109212, Test loss: 3.253852437590828\n",
      "--- 704.2929220199585 seconds for this training\n",
      "##############################################\n",
      "6th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.906/0.200 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.870/0.298\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.906/0.264 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.810/0.475\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.984/0.065 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.860/0.396\n",
      "----------------------------------------------\n",
      "*******************************Epoch    81: reducing learning rate of group 0 to 3.3333e-03.\n",
      "train loss is below  0.0075 , total epoch is:  100\n",
      "Best Val acc: 0.91, Val loss: 0.2731598323583603\n",
      "Test acc: 0.6696343779677113, Test loss: 1.4391431481297086\n",
      "--- 542.0764021873474 seconds for this training\n",
      "##############################################\n",
      "7th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.906/0.291 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.845/0.348\n",
      "----------------------------------------------\n",
      "[Epoch 51/300]    TRAIN acc/loss: 0.914/0.190 \n",
      "[Epoch 51/300] VAL   acc/loss: 0.885/0.296\n",
      "----------------------------------------------\n",
      "*******************************Epoch    72: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.977/0.082 ---\n",
      "[Epoch 76/300] VAL   acc/loss: 0.885/0.296\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.992/0.050 \n",
      "[Epoch 101/300] VAL   acc/loss: 0.885/0.392\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.984/0.028 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.910/0.345\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  128\n",
      "Best Val acc: 0.905, Val loss: 0.25500654220581054\n",
      "Test acc: 0.42746913580246915, Test loss: 2.9167588884674247\n",
      "--- 708.6524910926819 seconds for this training\n",
      "##############################################\n",
      "8th TRAIN FOLD BEGINS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Train size: 1404\n",
      "Val size: 200\n",
      "Test size: 8424\n",
      "[Epoch 26/300]    TRAIN acc/loss: 0.875/0.297 \n",
      "[Epoch 26/300] VAL   acc/loss: 0.830/0.381\n",
      "Best val_loss: 0.381056\n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51/300]    TRAIN acc/loss: 0.961/0.146 ---\n",
      "[Epoch 51/300] VAL   acc/loss: 0.870/0.368\n",
      "----------------------------------------------\n",
      "[Epoch 76/300]    TRAIN acc/loss: 0.914/0.195 \n",
      "[Epoch 76/300] VAL   acc/loss: 0.875/0.339\n",
      "----------------------------------------------\n",
      "[Epoch 101/300]    TRAIN acc/loss: 0.961/0.099 ---\n",
      "[Epoch 101/300] VAL   acc/loss: 0.900/0.343\n",
      "----------------------------------------------\n",
      "[Epoch 126/300]    TRAIN acc/loss: 0.953/0.098 \n",
      "[Epoch 126/300] VAL   acc/loss: 0.870/0.453\n",
      "----------------------------------------------\n",
      "*******************************Epoch   147: reducing learning rate of group 0 to 3.3333e-03.\n",
      "[Epoch 151/300]    TRAIN acc/loss: 0.977/0.028 \n",
      "[Epoch 151/300] VAL   acc/loss: 0.875/0.584\n",
      "----------------------------------------------\n",
      "train loss is below  0.0075 , total epoch is:  160\n",
      "Best Val acc: 0.885, Val loss: 0.26883246421813967\n",
      "Test acc: 0.6378205128205128, Test loss: 1.4378601280938752\n",
      "--- 888.9071836471558 seconds for this training\n",
      "   is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  is_iceberg_3  \\\n",
      "0         0.0      0.013611      0.007693  7.758599e-02  1.604709e-03   \n",
      "1         0.0      0.025149      0.025368  2.302089e-01  2.271132e-03   \n",
      "2         0.0      0.000000      0.000005  2.083130e-22  4.282011e-03   \n",
      "3         0.0      0.983793      0.999773  9.985307e-01  9.998800e-01   \n",
      "4         0.0      0.012430      0.999741  1.350930e-01  4.619642e-02   \n",
      "5         0.0      0.009449      0.999770  7.748591e-01  9.991234e-01   \n",
      "6         0.0      0.334007      0.050150  2.225048e-02  4.822319e-01   \n",
      "7         0.0      0.993336      0.999759  9.990069e-01  9.999009e-01   \n",
      "8         0.0      0.009321      0.000017  3.632260e-09  8.221752e-07   \n",
      "9         0.0      0.000005      0.000003  1.611793e-10  0.000000e+00   \n",
      "\n",
      "   is_iceberg_4  is_iceberg_5  is_iceberg_6  is_iceberg_7  \n",
      "0  2.228080e-02  2.837656e-03  8.966061e-04      0.001167  \n",
      "1  9.953691e-01  1.630019e-02  9.969969e-01      0.131280  \n",
      "2  2.682865e-15  1.027071e-14  0.000000e+00      0.000000  \n",
      "3  9.973069e-01  9.980369e-01  9.986534e-01      0.951348  \n",
      "4  9.973628e-01  4.044150e-01  9.979101e-01      0.045291  \n",
      "5  1.868624e-02  2.756268e-02  3.263239e-03      0.139982  \n",
      "6  3.065813e-01  1.761878e-01  2.868860e-03      0.209673  \n",
      "7  9.972664e-01  9.968304e-01  9.985830e-01      0.997643  \n",
      "8  6.630492e-04  5.161513e-04  5.272799e-04      0.000637  \n",
      "9  2.027065e-07  8.262112e-06  1.001335e-09      0.000051  \n",
      "              is_iceberg  is_iceberg_0  is_iceberg_1  is_iceberg_2  \\\n",
      "is_iceberg           NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0         NaN      1.000000      0.520593      0.683501   \n",
      "is_iceberg_1         NaN      0.520593      1.000000      0.591309   \n",
      "is_iceberg_2         NaN      0.683501      0.591309      1.000000   \n",
      "is_iceberg_3         NaN      0.570163      0.793238      0.594564   \n",
      "is_iceberg_4         NaN      0.414939      0.613985      0.560297   \n",
      "is_iceberg_5         NaN      0.524998      0.669408      0.472296   \n",
      "is_iceberg_6         NaN      0.502539      0.653134      0.540481   \n",
      "is_iceberg_7         NaN      0.794342      0.569951      0.647516   \n",
      "\n",
      "              is_iceberg_3  is_iceberg_4  is_iceberg_5  is_iceberg_6  \\\n",
      "is_iceberg             NaN           NaN           NaN           NaN   \n",
      "is_iceberg_0      0.570163      0.414939      0.524998      0.502539   \n",
      "is_iceberg_1      0.793238      0.613985      0.669408      0.653134   \n",
      "is_iceberg_2      0.594564      0.560297      0.472296      0.540481   \n",
      "is_iceberg_3      1.000000      0.557932      0.701424      0.603180   \n",
      "is_iceberg_4      0.557932      1.000000      0.450240      0.835607   \n",
      "is_iceberg_5      0.701424      0.450240      1.000000      0.588605   \n",
      "is_iceberg_6      0.603180      0.835607      0.588605      1.000000   \n",
      "is_iceberg_7      0.580124      0.493653      0.640896      0.624273   \n",
      "\n",
      "              is_iceberg_7  \n",
      "is_iceberg             NaN  \n",
      "is_iceberg_0      0.794342  \n",
      "is_iceberg_1      0.569951  \n",
      "is_iceberg_2      0.647516  \n",
      "is_iceberg_3      0.580124  \n",
      "is_iceberg_4      0.493653  \n",
      "is_iceberg_5      0.640896  \n",
      "is_iceberg_6      0.624273  \n",
      "is_iceberg_7      1.000000  \n",
      "\n",
      "epochForBestValLoss:  [39, 72, 51, 115, 53, 46, 73, 91]\n",
      "testAccuracy:  [0.71521842355175691, 0.46486229819563152, 0.53038936372269707, 0.51507597340930678, 0.3378442545109212, 0.6696343779677113, 0.42746913580246915, 0.63782051282051277]\n",
      "valLossForBestValLoss:  [0.18662371198729713, 0.2030802727375754, 0.19583210823547781, 0.16903007507324219, 0.15925306484103202, 0.27315983235836028, 0.25500654220581054, 0.26883246421813967]\n",
      "testLoss:  [1.0057658977091934, 3.214244129764162, 2.0038305050061074, 3.0279866845293038, 3.2538524375908282, 1.4391431481297086, 2.9167588884674247, 1.4378601280938752]\n",
      "\n",
      "Mean of epochForBestValLoss:  67.5\n",
      "Mean of testAccuracy:  0.537289292498\n",
      "Mean of valLossForBestValLoss:  0.213852258957\n",
      "Mean of testLoss:  2.28743022741\n"
     ]
    }
   ],
   "source": [
    "modelSelf = Model.VGG16_1\n",
    "\n",
    "hyperparameters = [\n",
    "\n",
    "     \n",
    "                    ({\"hidden\":128,\"dropout\" : 0.20}, 0.005,modelSelf,1e-6,\"average\",False),\n",
    "                            ({\"hidden\":1024,\"dropout\" : 0.35}, 0.005,modelSelf,1e-6,\"average\",False),\n",
    "                            ({\"hidden\":2048,\"dropout\" : 0.20}, 0.01,modelSelf,1e-6,\"substraction\",False),\n",
    "]\n",
    "\n",
    "\n",
    "for modelParameters, learningRate, modelSelf, weightDecay, channel3rd,smooth3rdChannel  in hyperparameters:\n",
    "\n",
    "\n",
    "    predictionsForTest, meanOfValLossForBestValLoss, _, _, _,_, _, _,_ = \\\n",
    "                    ValidationSystem.CVfor1Test.CV([band1,band2], [band1test,band2test], labelForTrain = labelForTrain, labelForTest = np.zeros(len(ids)),\n",
    "                            modelSelf= modelSelf, modelParameters = modelParameters, learningRate = learningRate,\n",
    "                            weightDecay = weightDecay, CVorPrediction=False, stopCVforValidation=10, \n",
    "                                                   kFoldNumberForValidation=8,logPerNumber=25,\n",
    "                                                   channel3rd=channel3rd,\n",
    "                            numberOfEpochs=300, batchsizeTrain = 128, batchsizeVal = 64, batchsizeTest = 64,\n",
    "                                                   inputBandNumber = 3,\n",
    "           smoothInput = False, smooth3rdChannel=smooth3rdChannel, pixelNormalization=False)\n",
    "\n",
    "\n",
    "    predictionsForTest.drop(['is_iceberg'],axis=1,inplace=True)\n",
    "    predictionsForTest.insert(loc=0, column='id', value=ids)\n",
    "\n",
    "    strParameters = \"\"\n",
    "    for (i, j) in modelParameters.items():\n",
    "        strParameters += str(i) + str(j)+\"-\"\n",
    "\n",
    "    fileName = \"../Predictions/{0:.4f}-{1}-parameters:{2}-lr:{3}-weightDecay{4}-smooth3rdChannel{5}.csv\".format(np.mean(meanOfValLossForBestValLoss),\n",
    "        modelSelf.__name__,  strParameters,learningRate, weightDecay,smooth3rdChannel)\n",
    "\n",
    "\n",
    "\n",
    "    predictionsForTest.to_csv(fileName, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>is_iceberg_0</th>\n",
       "      <th>is_iceberg_1</th>\n",
       "      <th>is_iceberg_2</th>\n",
       "      <th>is_iceberg_3</th>\n",
       "      <th>is_iceberg_4</th>\n",
       "      <th>is_iceberg_5</th>\n",
       "      <th>is_iceberg_6</th>\n",
       "      <th>is_iceberg_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5941774d</td>\n",
       "      <td>1.361060e-02</td>\n",
       "      <td>7.692950e-03</td>\n",
       "      <td>7.758599e-02</td>\n",
       "      <td>1.604709e-03</td>\n",
       "      <td>2.228080e-02</td>\n",
       "      <td>2.837656e-03</td>\n",
       "      <td>8.966061e-04</td>\n",
       "      <td>1.167400e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4023181e</td>\n",
       "      <td>2.514934e-02</td>\n",
       "      <td>2.536800e-02</td>\n",
       "      <td>2.302089e-01</td>\n",
       "      <td>2.271132e-03</td>\n",
       "      <td>9.953691e-01</td>\n",
       "      <td>1.630019e-02</td>\n",
       "      <td>9.969969e-01</td>\n",
       "      <td>1.312800e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b20200e4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.597272e-06</td>\n",
       "      <td>2.083130e-22</td>\n",
       "      <td>4.282011e-03</td>\n",
       "      <td>2.682865e-15</td>\n",
       "      <td>1.027071e-14</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>9.837928e-01</td>\n",
       "      <td>9.997733e-01</td>\n",
       "      <td>9.985307e-01</td>\n",
       "      <td>9.998800e-01</td>\n",
       "      <td>9.973069e-01</td>\n",
       "      <td>9.980369e-01</td>\n",
       "      <td>9.986534e-01</td>\n",
       "      <td>9.513484e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>1.243004e-02</td>\n",
       "      <td>9.997415e-01</td>\n",
       "      <td>1.350930e-01</td>\n",
       "      <td>4.619642e-02</td>\n",
       "      <td>9.973628e-01</td>\n",
       "      <td>4.044150e-01</td>\n",
       "      <td>9.979101e-01</td>\n",
       "      <td>4.529118e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a8d9b1fd</td>\n",
       "      <td>9.449063e-03</td>\n",
       "      <td>9.997702e-01</td>\n",
       "      <td>7.748591e-01</td>\n",
       "      <td>9.991234e-01</td>\n",
       "      <td>1.868624e-02</td>\n",
       "      <td>2.756268e-02</td>\n",
       "      <td>3.263239e-03</td>\n",
       "      <td>1.399824e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29e7727e</td>\n",
       "      <td>3.340073e-01</td>\n",
       "      <td>5.015022e-02</td>\n",
       "      <td>2.225048e-02</td>\n",
       "      <td>4.822319e-01</td>\n",
       "      <td>3.065813e-01</td>\n",
       "      <td>1.761878e-01</td>\n",
       "      <td>2.868860e-03</td>\n",
       "      <td>2.096731e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>92a51ffb</td>\n",
       "      <td>9.933356e-01</td>\n",
       "      <td>9.997595e-01</td>\n",
       "      <td>9.990069e-01</td>\n",
       "      <td>9.999009e-01</td>\n",
       "      <td>9.972664e-01</td>\n",
       "      <td>9.968304e-01</td>\n",
       "      <td>9.985830e-01</td>\n",
       "      <td>9.976427e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c769ac97</td>\n",
       "      <td>9.320790e-03</td>\n",
       "      <td>1.736006e-05</td>\n",
       "      <td>3.632260e-09</td>\n",
       "      <td>8.221752e-07</td>\n",
       "      <td>6.630492e-04</td>\n",
       "      <td>5.161513e-04</td>\n",
       "      <td>5.272799e-04</td>\n",
       "      <td>6.369714e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aee0547d</td>\n",
       "      <td>5.073208e-06</td>\n",
       "      <td>3.358853e-06</td>\n",
       "      <td>1.611793e-10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.027065e-07</td>\n",
       "      <td>8.262112e-06</td>\n",
       "      <td>1.001335e-09</td>\n",
       "      <td>5.126987e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>565b28ac</td>\n",
       "      <td>1.181990e-02</td>\n",
       "      <td>5.229271e-07</td>\n",
       "      <td>6.182345e-14</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.450930e-09</td>\n",
       "      <td>4.356955e-33</td>\n",
       "      <td>2.837172e-09</td>\n",
       "      <td>3.840890e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e04e9775</td>\n",
       "      <td>2.707265e-02</td>\n",
       "      <td>1.706201e-01</td>\n",
       "      <td>2.263829e-01</td>\n",
       "      <td>5.905780e-01</td>\n",
       "      <td>9.927514e-01</td>\n",
       "      <td>7.879933e-01</td>\n",
       "      <td>9.924999e-01</td>\n",
       "      <td>3.421683e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8e8161d1</td>\n",
       "      <td>7.709431e-03</td>\n",
       "      <td>7.935238e-04</td>\n",
       "      <td>1.805289e-02</td>\n",
       "      <td>7.634669e-04</td>\n",
       "      <td>3.013640e-03</td>\n",
       "      <td>1.795544e-07</td>\n",
       "      <td>4.159452e-04</td>\n",
       "      <td>1.639317e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4cf4d256</td>\n",
       "      <td>1.446103e-01</td>\n",
       "      <td>9.997761e-01</td>\n",
       "      <td>9.817293e-01</td>\n",
       "      <td>9.938916e-01</td>\n",
       "      <td>9.974255e-01</td>\n",
       "      <td>9.983334e-01</td>\n",
       "      <td>9.981945e-01</td>\n",
       "      <td>8.741388e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>139e5324</td>\n",
       "      <td>9.243714e-01</td>\n",
       "      <td>9.973261e-01</td>\n",
       "      <td>9.956371e-01</td>\n",
       "      <td>9.930016e-01</td>\n",
       "      <td>9.919541e-01</td>\n",
       "      <td>1.174629e-02</td>\n",
       "      <td>6.735176e-01</td>\n",
       "      <td>5.014924e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>f156976f</td>\n",
       "      <td>1.890093e-02</td>\n",
       "      <td>2.830625e-03</td>\n",
       "      <td>1.209375e-02</td>\n",
       "      <td>3.223673e-03</td>\n",
       "      <td>1.013724e-02</td>\n",
       "      <td>2.978290e-04</td>\n",
       "      <td>3.053932e-04</td>\n",
       "      <td>1.865841e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>68a117cc</td>\n",
       "      <td>1.743717e-02</td>\n",
       "      <td>9.949019e-01</td>\n",
       "      <td>9.734693e-01</td>\n",
       "      <td>4.560975e-01</td>\n",
       "      <td>9.969388e-01</td>\n",
       "      <td>4.874426e-01</td>\n",
       "      <td>9.973533e-01</td>\n",
       "      <td>4.527628e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>d9aa7a56</td>\n",
       "      <td>1.041089e-02</td>\n",
       "      <td>9.967300e-01</td>\n",
       "      <td>2.761214e-01</td>\n",
       "      <td>6.678775e-03</td>\n",
       "      <td>9.921233e-01</td>\n",
       "      <td>6.651817e-03</td>\n",
       "      <td>9.047630e-02</td>\n",
       "      <td>5.753524e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9005b143</td>\n",
       "      <td>2.960910e-01</td>\n",
       "      <td>4.823790e-02</td>\n",
       "      <td>7.061662e-01</td>\n",
       "      <td>1.149588e-02</td>\n",
       "      <td>9.904365e-01</td>\n",
       "      <td>7.194176e-03</td>\n",
       "      <td>7.506327e-02</td>\n",
       "      <td>8.701750e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5f6d3988</td>\n",
       "      <td>8.703553e-01</td>\n",
       "      <td>9.997633e-01</td>\n",
       "      <td>9.639842e-01</td>\n",
       "      <td>9.998766e-01</td>\n",
       "      <td>9.973727e-01</td>\n",
       "      <td>9.981152e-01</td>\n",
       "      <td>9.983658e-01</td>\n",
       "      <td>9.907997e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9ad70954</td>\n",
       "      <td>8.415639e-03</td>\n",
       "      <td>9.997742e-01</td>\n",
       "      <td>5.355053e-02</td>\n",
       "      <td>9.998920e-01</td>\n",
       "      <td>9.974088e-01</td>\n",
       "      <td>9.974522e-01</td>\n",
       "      <td>9.979583e-01</td>\n",
       "      <td>2.476365e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b9087b9e</td>\n",
       "      <td>1.290754e-01</td>\n",
       "      <td>9.693173e-01</td>\n",
       "      <td>8.645493e-01</td>\n",
       "      <td>9.881172e-01</td>\n",
       "      <td>4.879707e-01</td>\n",
       "      <td>1.000060e-01</td>\n",
       "      <td>1.497394e-01</td>\n",
       "      <td>9.007128e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>a39a1427</td>\n",
       "      <td>3.984890e-01</td>\n",
       "      <td>1.322011e-01</td>\n",
       "      <td>3.850875e-01</td>\n",
       "      <td>9.317352e-01</td>\n",
       "      <td>9.968603e-01</td>\n",
       "      <td>9.225287e-03</td>\n",
       "      <td>6.009689e-01</td>\n",
       "      <td>4.231315e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>82fbe8ed</td>\n",
       "      <td>2.826881e-02</td>\n",
       "      <td>5.940871e-01</td>\n",
       "      <td>4.055931e-02</td>\n",
       "      <td>1.086801e-03</td>\n",
       "      <td>3.799136e-02</td>\n",
       "      <td>4.660891e-02</td>\n",
       "      <td>2.152534e-03</td>\n",
       "      <td>3.124978e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1fae4879</td>\n",
       "      <td>8.452618e-03</td>\n",
       "      <td>4.901630e-06</td>\n",
       "      <td>8.288635e-19</td>\n",
       "      <td>3.715178e-17</td>\n",
       "      <td>1.902359e-06</td>\n",
       "      <td>7.446603e-32</td>\n",
       "      <td>3.487754e-05</td>\n",
       "      <td>3.587224e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6dd8f13d</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.082432e-08</td>\n",
       "      <td>8.399620e-04</td>\n",
       "      <td>3.562892e-05</td>\n",
       "      <td>2.009183e-02</td>\n",
       "      <td>1.335276e-32</td>\n",
       "      <td>2.672307e-20</td>\n",
       "      <td>1.294141e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bbad5958</td>\n",
       "      <td>3.820826e-02</td>\n",
       "      <td>4.754743e-01</td>\n",
       "      <td>2.740164e-02</td>\n",
       "      <td>1.233201e-01</td>\n",
       "      <td>1.345506e-01</td>\n",
       "      <td>1.333941e-02</td>\n",
       "      <td>8.799661e-03</td>\n",
       "      <td>7.591689e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>54527583</td>\n",
       "      <td>4.039735e-02</td>\n",
       "      <td>9.757280e-01</td>\n",
       "      <td>9.990106e-01</td>\n",
       "      <td>9.781568e-01</td>\n",
       "      <td>9.973404e-01</td>\n",
       "      <td>6.090350e-02</td>\n",
       "      <td>9.969325e-01</td>\n",
       "      <td>1.590766e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>be8fa29c</td>\n",
       "      <td>1.170930e-02</td>\n",
       "      <td>3.495133e-02</td>\n",
       "      <td>2.048504e-01</td>\n",
       "      <td>7.371597e-03</td>\n",
       "      <td>9.963325e-01</td>\n",
       "      <td>1.304162e-02</td>\n",
       "      <td>9.133437e-01</td>\n",
       "      <td>4.385111e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>81a3328f</td>\n",
       "      <td>9.941209e-01</td>\n",
       "      <td>9.997219e-01</td>\n",
       "      <td>9.991438e-01</td>\n",
       "      <td>9.998379e-01</td>\n",
       "      <td>9.972953e-01</td>\n",
       "      <td>9.938418e-01</td>\n",
       "      <td>9.979875e-01</td>\n",
       "      <td>9.970540e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>8ae30ce6</td>\n",
       "      <td>2.246979e-09</td>\n",
       "      <td>8.975557e-04</td>\n",
       "      <td>2.364535e-01</td>\n",
       "      <td>4.382878e-03</td>\n",
       "      <td>3.377735e-01</td>\n",
       "      <td>5.430440e-04</td>\n",
       "      <td>4.245362e-02</td>\n",
       "      <td>4.897941e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8395</th>\n",
       "      <td>de27ed88</td>\n",
       "      <td>8.407562e-05</td>\n",
       "      <td>4.152103e-06</td>\n",
       "      <td>9.550843e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.195998e-14</td>\n",
       "      <td>1.045552e-22</td>\n",
       "      <td>6.016916e-11</td>\n",
       "      <td>1.763369e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8396</th>\n",
       "      <td>66d5196f</td>\n",
       "      <td>9.650595e-03</td>\n",
       "      <td>9.990256e-01</td>\n",
       "      <td>4.347247e-01</td>\n",
       "      <td>2.949498e-02</td>\n",
       "      <td>9.972996e-01</td>\n",
       "      <td>2.950652e-01</td>\n",
       "      <td>9.970830e-01</td>\n",
       "      <td>6.311727e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>d85f1858</td>\n",
       "      <td>3.610437e-01</td>\n",
       "      <td>3.450558e-03</td>\n",
       "      <td>9.827129e-01</td>\n",
       "      <td>6.512622e-03</td>\n",
       "      <td>9.964529e-01</td>\n",
       "      <td>1.038386e-02</td>\n",
       "      <td>9.982662e-01</td>\n",
       "      <td>9.865996e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>16dcb33a</td>\n",
       "      <td>1.025421e-01</td>\n",
       "      <td>9.983296e-01</td>\n",
       "      <td>3.381114e-01</td>\n",
       "      <td>2.438265e-02</td>\n",
       "      <td>9.973503e-01</td>\n",
       "      <td>8.862513e-01</td>\n",
       "      <td>9.985216e-01</td>\n",
       "      <td>7.180159e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>eca3158e</td>\n",
       "      <td>1.133895e-01</td>\n",
       "      <td>2.651243e-02</td>\n",
       "      <td>9.026173e-01</td>\n",
       "      <td>9.833757e-01</td>\n",
       "      <td>3.493567e-01</td>\n",
       "      <td>2.996483e-03</td>\n",
       "      <td>1.030611e-02</td>\n",
       "      <td>1.199668e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>08daeee6</td>\n",
       "      <td>8.160357e-03</td>\n",
       "      <td>1.143001e-05</td>\n",
       "      <td>4.852698e-04</td>\n",
       "      <td>6.384836e-11</td>\n",
       "      <td>9.811275e-04</td>\n",
       "      <td>1.020769e-09</td>\n",
       "      <td>2.963773e-04</td>\n",
       "      <td>1.219398e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>e9c513ee</td>\n",
       "      <td>1.324033e-02</td>\n",
       "      <td>2.914131e-03</td>\n",
       "      <td>8.541294e-03</td>\n",
       "      <td>2.068112e-02</td>\n",
       "      <td>4.255864e-03</td>\n",
       "      <td>3.706852e-03</td>\n",
       "      <td>5.816530e-04</td>\n",
       "      <td>7.338363e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>b1519fa6</td>\n",
       "      <td>3.742208e-02</td>\n",
       "      <td>9.987914e-01</td>\n",
       "      <td>7.154235e-01</td>\n",
       "      <td>3.690787e-02</td>\n",
       "      <td>9.972886e-01</td>\n",
       "      <td>1.664778e-01</td>\n",
       "      <td>9.987696e-01</td>\n",
       "      <td>9.955077e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>dfc89540</td>\n",
       "      <td>1.688296e-01</td>\n",
       "      <td>9.997678e-01</td>\n",
       "      <td>2.328824e-01</td>\n",
       "      <td>9.998927e-01</td>\n",
       "      <td>9.974038e-01</td>\n",
       "      <td>9.975206e-01</td>\n",
       "      <td>9.987454e-01</td>\n",
       "      <td>9.436557e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>8fd8c0e9</td>\n",
       "      <td>9.902277e-01</td>\n",
       "      <td>7.929748e-01</td>\n",
       "      <td>9.992121e-01</td>\n",
       "      <td>9.999367e-01</td>\n",
       "      <td>9.972787e-01</td>\n",
       "      <td>7.751931e-02</td>\n",
       "      <td>9.985619e-01</td>\n",
       "      <td>9.977797e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>45df6347</td>\n",
       "      <td>2.714002e-02</td>\n",
       "      <td>7.534067e-04</td>\n",
       "      <td>1.482660e-02</td>\n",
       "      <td>2.053697e-06</td>\n",
       "      <td>3.386674e-03</td>\n",
       "      <td>4.469687e-04</td>\n",
       "      <td>8.516443e-04</td>\n",
       "      <td>1.440227e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>bf7928d7</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.691701e-04</td>\n",
       "      <td>6.334125e-01</td>\n",
       "      <td>9.948952e-05</td>\n",
       "      <td>1.419560e-02</td>\n",
       "      <td>1.535718e-36</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>7b587c05</td>\n",
       "      <td>7.796473e-03</td>\n",
       "      <td>4.351010e-03</td>\n",
       "      <td>1.667706e-04</td>\n",
       "      <td>2.580809e-06</td>\n",
       "      <td>3.440527e-03</td>\n",
       "      <td>1.660325e-03</td>\n",
       "      <td>1.495053e-04</td>\n",
       "      <td>4.755086e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>c2834388</td>\n",
       "      <td>1.723425e-01</td>\n",
       "      <td>9.994350e-01</td>\n",
       "      <td>9.231521e-01</td>\n",
       "      <td>2.106495e-01</td>\n",
       "      <td>9.838803e-01</td>\n",
       "      <td>1.035638e-02</td>\n",
       "      <td>9.607539e-01</td>\n",
       "      <td>2.458363e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>146143c3</td>\n",
       "      <td>9.944735e-01</td>\n",
       "      <td>9.994143e-01</td>\n",
       "      <td>9.867474e-01</td>\n",
       "      <td>9.992471e-01</td>\n",
       "      <td>8.226143e-01</td>\n",
       "      <td>9.656628e-01</td>\n",
       "      <td>9.984518e-01</td>\n",
       "      <td>9.978428e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>d59aee00</td>\n",
       "      <td>8.930486e-03</td>\n",
       "      <td>8.667950e-06</td>\n",
       "      <td>3.587477e-14</td>\n",
       "      <td>5.408604e-17</td>\n",
       "      <td>5.408268e-07</td>\n",
       "      <td>2.973836e-35</td>\n",
       "      <td>2.852641e-04</td>\n",
       "      <td>1.578641e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>cbc0b93b</td>\n",
       "      <td>1.619501e-02</td>\n",
       "      <td>9.956867e-01</td>\n",
       "      <td>2.453112e-01</td>\n",
       "      <td>9.897988e-01</td>\n",
       "      <td>9.972951e-01</td>\n",
       "      <td>1.009260e-01</td>\n",
       "      <td>9.977532e-01</td>\n",
       "      <td>1.028162e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>088e2ff7</td>\n",
       "      <td>9.679851e-01</td>\n",
       "      <td>9.989257e-01</td>\n",
       "      <td>9.958526e-01</td>\n",
       "      <td>9.984406e-01</td>\n",
       "      <td>9.638264e-01</td>\n",
       "      <td>9.529585e-01</td>\n",
       "      <td>9.688869e-01</td>\n",
       "      <td>9.958396e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>673d33cd</td>\n",
       "      <td>6.105750e-11</td>\n",
       "      <td>1.351603e-02</td>\n",
       "      <td>4.030677e-01</td>\n",
       "      <td>1.933498e-02</td>\n",
       "      <td>9.783391e-01</td>\n",
       "      <td>1.487906e-02</td>\n",
       "      <td>8.107194e-01</td>\n",
       "      <td>7.904815e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>674b031e</td>\n",
       "      <td>8.220435e-14</td>\n",
       "      <td>5.551099e-06</td>\n",
       "      <td>5.364386e-02</td>\n",
       "      <td>3.618178e-04</td>\n",
       "      <td>5.814698e-02</td>\n",
       "      <td>4.360871e-06</td>\n",
       "      <td>1.241290e-02</td>\n",
       "      <td>3.464474e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>43db4207</td>\n",
       "      <td>7.784171e-01</td>\n",
       "      <td>9.996763e-01</td>\n",
       "      <td>9.516687e-01</td>\n",
       "      <td>9.997771e-01</td>\n",
       "      <td>9.971147e-01</td>\n",
       "      <td>3.196889e-01</td>\n",
       "      <td>9.971770e-01</td>\n",
       "      <td>5.005422e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>156855e1</td>\n",
       "      <td>2.266588e-03</td>\n",
       "      <td>4.400222e-08</td>\n",
       "      <td>1.341536e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.171331e-09</td>\n",
       "      <td>6.137124e-33</td>\n",
       "      <td>1.834772e-19</td>\n",
       "      <td>1.416029e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8417</th>\n",
       "      <td>ac96cfb0</td>\n",
       "      <td>8.514971e-02</td>\n",
       "      <td>9.744519e-01</td>\n",
       "      <td>2.829764e-01</td>\n",
       "      <td>1.787541e-02</td>\n",
       "      <td>9.940995e-01</td>\n",
       "      <td>2.830291e-03</td>\n",
       "      <td>2.301601e-01</td>\n",
       "      <td>2.422758e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>fe45aef5</td>\n",
       "      <td>9.559895e-01</td>\n",
       "      <td>9.973817e-01</td>\n",
       "      <td>9.959607e-01</td>\n",
       "      <td>9.987028e-01</td>\n",
       "      <td>8.731603e-01</td>\n",
       "      <td>9.626433e-01</td>\n",
       "      <td>9.595554e-01</td>\n",
       "      <td>9.738249e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>16ee9b50</td>\n",
       "      <td>4.619607e-19</td>\n",
       "      <td>9.437540e-12</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.521018e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.543483e-21</td>\n",
       "      <td>2.044700e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8420</th>\n",
       "      <td>5a599eb7</td>\n",
       "      <td>2.448886e-02</td>\n",
       "      <td>2.376269e-03</td>\n",
       "      <td>6.495141e-02</td>\n",
       "      <td>2.522595e-03</td>\n",
       "      <td>9.954799e-01</td>\n",
       "      <td>9.542818e-03</td>\n",
       "      <td>9.912600e-01</td>\n",
       "      <td>4.787170e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>df30d6dd</td>\n",
       "      <td>4.305770e-02</td>\n",
       "      <td>2.289718e-03</td>\n",
       "      <td>2.362510e-02</td>\n",
       "      <td>4.921758e-04</td>\n",
       "      <td>4.577188e-02</td>\n",
       "      <td>8.844365e-03</td>\n",
       "      <td>1.174983e-02</td>\n",
       "      <td>2.767398e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8422</th>\n",
       "      <td>18af95b1</td>\n",
       "      <td>9.876149e-01</td>\n",
       "      <td>9.996566e-01</td>\n",
       "      <td>9.962392e-01</td>\n",
       "      <td>9.996891e-01</td>\n",
       "      <td>9.973603e-01</td>\n",
       "      <td>9.980977e-01</td>\n",
       "      <td>9.993029e-01</td>\n",
       "      <td>9.975718e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8423</th>\n",
       "      <td>27d788c8</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.446447e-03</td>\n",
       "      <td>9.072924e-01</td>\n",
       "      <td>1.233182e-04</td>\n",
       "      <td>2.002802e-01</td>\n",
       "      <td>3.244149e-38</td>\n",
       "      <td>1.687805e-17</td>\n",
       "      <td>7.518963e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8424 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  is_iceberg_0  is_iceberg_1  is_iceberg_2  is_iceberg_3  \\\n",
       "0     5941774d  1.361060e-02  7.692950e-03  7.758599e-02  1.604709e-03   \n",
       "1     4023181e  2.514934e-02  2.536800e-02  2.302089e-01  2.271132e-03   \n",
       "2     b20200e4  0.000000e+00  4.597272e-06  2.083130e-22  4.282011e-03   \n",
       "3     e7f018bb  9.837928e-01  9.997733e-01  9.985307e-01  9.998800e-01   \n",
       "4     4371c8c3  1.243004e-02  9.997415e-01  1.350930e-01  4.619642e-02   \n",
       "5     a8d9b1fd  9.449063e-03  9.997702e-01  7.748591e-01  9.991234e-01   \n",
       "6     29e7727e  3.340073e-01  5.015022e-02  2.225048e-02  4.822319e-01   \n",
       "7     92a51ffb  9.933356e-01  9.997595e-01  9.990069e-01  9.999009e-01   \n",
       "8     c769ac97  9.320790e-03  1.736006e-05  3.632260e-09  8.221752e-07   \n",
       "9     aee0547d  5.073208e-06  3.358853e-06  1.611793e-10  0.000000e+00   \n",
       "10    565b28ac  1.181990e-02  5.229271e-07  6.182345e-14  0.000000e+00   \n",
       "11    e04e9775  2.707265e-02  1.706201e-01  2.263829e-01  5.905780e-01   \n",
       "12    8e8161d1  7.709431e-03  7.935238e-04  1.805289e-02  7.634669e-04   \n",
       "13    4cf4d256  1.446103e-01  9.997761e-01  9.817293e-01  9.938916e-01   \n",
       "14    139e5324  9.243714e-01  9.973261e-01  9.956371e-01  9.930016e-01   \n",
       "15    f156976f  1.890093e-02  2.830625e-03  1.209375e-02  3.223673e-03   \n",
       "16    68a117cc  1.743717e-02  9.949019e-01  9.734693e-01  4.560975e-01   \n",
       "17    d9aa7a56  1.041089e-02  9.967300e-01  2.761214e-01  6.678775e-03   \n",
       "18    9005b143  2.960910e-01  4.823790e-02  7.061662e-01  1.149588e-02   \n",
       "19    5f6d3988  8.703553e-01  9.997633e-01  9.639842e-01  9.998766e-01   \n",
       "20    9ad70954  8.415639e-03  9.997742e-01  5.355053e-02  9.998920e-01   \n",
       "21    b9087b9e  1.290754e-01  9.693173e-01  8.645493e-01  9.881172e-01   \n",
       "22    a39a1427  3.984890e-01  1.322011e-01  3.850875e-01  9.317352e-01   \n",
       "23    82fbe8ed  2.826881e-02  5.940871e-01  4.055931e-02  1.086801e-03   \n",
       "24    1fae4879  8.452618e-03  4.901630e-06  8.288635e-19  3.715178e-17   \n",
       "25    6dd8f13d  0.000000e+00  1.082432e-08  8.399620e-04  3.562892e-05   \n",
       "26    bbad5958  3.820826e-02  4.754743e-01  2.740164e-02  1.233201e-01   \n",
       "27    54527583  4.039735e-02  9.757280e-01  9.990106e-01  9.781568e-01   \n",
       "28    be8fa29c  1.170930e-02  3.495133e-02  2.048504e-01  7.371597e-03   \n",
       "29    81a3328f  9.941209e-01  9.997219e-01  9.991438e-01  9.998379e-01   \n",
       "...        ...           ...           ...           ...           ...   \n",
       "8394  8ae30ce6  2.246979e-09  8.975557e-04  2.364535e-01  4.382878e-03   \n",
       "8395  de27ed88  8.407562e-05  4.152103e-06  9.550843e-16  0.000000e+00   \n",
       "8396  66d5196f  9.650595e-03  9.990256e-01  4.347247e-01  2.949498e-02   \n",
       "8397  d85f1858  3.610437e-01  3.450558e-03  9.827129e-01  6.512622e-03   \n",
       "8398  16dcb33a  1.025421e-01  9.983296e-01  3.381114e-01  2.438265e-02   \n",
       "8399  eca3158e  1.133895e-01  2.651243e-02  9.026173e-01  9.833757e-01   \n",
       "8400  08daeee6  8.160357e-03  1.143001e-05  4.852698e-04  6.384836e-11   \n",
       "8401  e9c513ee  1.324033e-02  2.914131e-03  8.541294e-03  2.068112e-02   \n",
       "8402  b1519fa6  3.742208e-02  9.987914e-01  7.154235e-01  3.690787e-02   \n",
       "8403  dfc89540  1.688296e-01  9.997678e-01  2.328824e-01  9.998927e-01   \n",
       "8404  8fd8c0e9  9.902277e-01  7.929748e-01  9.992121e-01  9.999367e-01   \n",
       "8405  45df6347  2.714002e-02  7.534067e-04  1.482660e-02  2.053697e-06   \n",
       "8406  bf7928d7  0.000000e+00  9.691701e-04  6.334125e-01  9.948952e-05   \n",
       "8407  7b587c05  7.796473e-03  4.351010e-03  1.667706e-04  2.580809e-06   \n",
       "8408  c2834388  1.723425e-01  9.994350e-01  9.231521e-01  2.106495e-01   \n",
       "8409  146143c3  9.944735e-01  9.994143e-01  9.867474e-01  9.992471e-01   \n",
       "8410  d59aee00  8.930486e-03  8.667950e-06  3.587477e-14  5.408604e-17   \n",
       "8411  cbc0b93b  1.619501e-02  9.956867e-01  2.453112e-01  9.897988e-01   \n",
       "8412  088e2ff7  9.679851e-01  9.989257e-01  9.958526e-01  9.984406e-01   \n",
       "8413  673d33cd  6.105750e-11  1.351603e-02  4.030677e-01  1.933498e-02   \n",
       "8414  674b031e  8.220435e-14  5.551099e-06  5.364386e-02  3.618178e-04   \n",
       "8415  43db4207  7.784171e-01  9.996763e-01  9.516687e-01  9.997771e-01   \n",
       "8416  156855e1  2.266588e-03  4.400222e-08  1.341536e-16  0.000000e+00   \n",
       "8417  ac96cfb0  8.514971e-02  9.744519e-01  2.829764e-01  1.787541e-02   \n",
       "8418  fe45aef5  9.559895e-01  9.973817e-01  9.959607e-01  9.987028e-01   \n",
       "8419  16ee9b50  4.619607e-19  9.437540e-12  0.000000e+00  0.000000e+00   \n",
       "8420  5a599eb7  2.448886e-02  2.376269e-03  6.495141e-02  2.522595e-03   \n",
       "8421  df30d6dd  4.305770e-02  2.289718e-03  2.362510e-02  4.921758e-04   \n",
       "8422  18af95b1  9.876149e-01  9.996566e-01  9.962392e-01  9.996891e-01   \n",
       "8423  27d788c8  0.000000e+00  1.446447e-03  9.072924e-01  1.233182e-04   \n",
       "\n",
       "      is_iceberg_4  is_iceberg_5  is_iceberg_6  is_iceberg_7  \n",
       "0     2.228080e-02  2.837656e-03  8.966061e-04  1.167400e-03  \n",
       "1     9.953691e-01  1.630019e-02  9.969969e-01  1.312800e-01  \n",
       "2     2.682865e-15  1.027071e-14  0.000000e+00  0.000000e+00  \n",
       "3     9.973069e-01  9.980369e-01  9.986534e-01  9.513484e-01  \n",
       "4     9.973628e-01  4.044150e-01  9.979101e-01  4.529118e-02  \n",
       "5     1.868624e-02  2.756268e-02  3.263239e-03  1.399824e-01  \n",
       "6     3.065813e-01  1.761878e-01  2.868860e-03  2.096731e-01  \n",
       "7     9.972664e-01  9.968304e-01  9.985830e-01  9.976427e-01  \n",
       "8     6.630492e-04  5.161513e-04  5.272799e-04  6.369714e-04  \n",
       "9     2.027065e-07  8.262112e-06  1.001335e-09  5.126987e-05  \n",
       "10    1.450930e-09  4.356955e-33  2.837172e-09  3.840890e-05  \n",
       "11    9.927514e-01  7.879933e-01  9.924999e-01  3.421683e-01  \n",
       "12    3.013640e-03  1.795544e-07  4.159452e-04  1.639317e-03  \n",
       "13    9.974255e-01  9.983334e-01  9.981945e-01  8.741388e-01  \n",
       "14    9.919541e-01  1.174629e-02  6.735176e-01  5.014924e-01  \n",
       "15    1.013724e-02  2.978290e-04  3.053932e-04  1.865841e-02  \n",
       "16    9.969388e-01  4.874426e-01  9.973533e-01  4.527628e-02  \n",
       "17    9.921233e-01  6.651817e-03  9.047630e-02  5.753524e-03  \n",
       "18    9.904365e-01  7.194176e-03  7.506327e-02  8.701750e-03  \n",
       "19    9.973727e-01  9.981152e-01  9.983658e-01  9.907997e-01  \n",
       "20    9.974088e-01  9.974522e-01  9.979583e-01  2.476365e-03  \n",
       "21    4.879707e-01  1.000060e-01  1.497394e-01  9.007128e-02  \n",
       "22    9.968603e-01  9.225287e-03  6.009689e-01  4.231315e-02  \n",
       "23    3.799136e-02  4.660891e-02  2.152534e-03  3.124978e-01  \n",
       "24    1.902359e-06  7.446603e-32  3.487754e-05  3.587224e-04  \n",
       "25    2.009183e-02  1.335276e-32  2.672307e-20  1.294141e-22  \n",
       "26    1.345506e-01  1.333941e-02  8.799661e-03  7.591689e-02  \n",
       "27    9.973404e-01  6.090350e-02  9.969325e-01  1.590766e-02  \n",
       "28    9.963325e-01  1.304162e-02  9.133437e-01  4.385111e-03  \n",
       "29    9.972953e-01  9.938418e-01  9.979875e-01  9.970540e-01  \n",
       "...            ...           ...           ...           ...  \n",
       "8394  3.377735e-01  5.430440e-04  4.245362e-02  4.897941e-04  \n",
       "8395  2.195998e-14  1.045552e-22  6.016916e-11  1.763369e-04  \n",
       "8396  9.972996e-01  2.950652e-01  9.970830e-01  6.311727e-02  \n",
       "8397  9.964529e-01  1.038386e-02  9.982662e-01  9.865996e-01  \n",
       "8398  9.973503e-01  8.862513e-01  9.985216e-01  7.180159e-01  \n",
       "8399  3.493567e-01  2.996483e-03  1.030611e-02  1.199668e-02  \n",
       "8400  9.811275e-04  1.020769e-09  2.963773e-04  1.219398e-02  \n",
       "8401  4.255864e-03  3.706852e-03  5.816530e-04  7.338363e-02  \n",
       "8402  9.972886e-01  1.664778e-01  9.987696e-01  9.955077e-01  \n",
       "8403  9.974038e-01  9.975206e-01  9.987454e-01  9.436557e-01  \n",
       "8404  9.972787e-01  7.751931e-02  9.985619e-01  9.977797e-01  \n",
       "8405  3.386674e-03  4.469687e-04  8.516443e-04  1.440227e-01  \n",
       "8406  1.419560e-02  1.535718e-36  0.000000e+00  0.000000e+00  \n",
       "8407  3.440527e-03  1.660325e-03  1.495053e-04  4.755086e-04  \n",
       "8408  9.838803e-01  1.035638e-02  9.607539e-01  2.458363e-01  \n",
       "8409  8.226143e-01  9.656628e-01  9.984518e-01  9.978428e-01  \n",
       "8410  5.408268e-07  2.973836e-35  2.852641e-04  1.578641e-03  \n",
       "8411  9.972951e-01  1.009260e-01  9.977532e-01  1.028162e-01  \n",
       "8412  9.638264e-01  9.529585e-01  9.688869e-01  9.958396e-01  \n",
       "8413  9.783391e-01  1.487906e-02  8.107194e-01  7.904815e-03  \n",
       "8414  5.814698e-02  4.360871e-06  1.241290e-02  3.464474e-04  \n",
       "8415  9.971147e-01  3.196889e-01  9.971770e-01  5.005422e-01  \n",
       "8416  2.171331e-09  6.137124e-33  1.834772e-19  1.416029e-19  \n",
       "8417  9.940995e-01  2.830291e-03  2.301601e-01  2.422758e-03  \n",
       "8418  8.731603e-01  9.626433e-01  9.595554e-01  9.738249e-01  \n",
       "8419  2.521018e-16  0.000000e+00  7.543483e-21  2.044700e-13  \n",
       "8420  9.954799e-01  9.542818e-03  9.912600e-01  4.787170e-01  \n",
       "8421  4.577188e-02  8.844365e-03  1.174983e-02  2.767398e-03  \n",
       "8422  9.973603e-01  9.980977e-01  9.993029e-01  9.975718e-01  \n",
       "8423  2.002802e-01  3.244149e-38  1.687805e-17  7.518963e-03  \n",
       "\n",
       "[8424 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsForTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
